# VCB-S 网站爬虫使用说明

## 功能特性

1. **增量更新**：智能检测新帖子，只爬取未收录的内容
2. **滑动窗口**：分批处理帖子，避免内存占用过大
3. **并行处理**：支持多线程并行爬取，提高效率
4. **鲁棒性强**：采用多种方法提取信息，确保稳定性
5. **实时保存**：数据实时写入数据库和Excel，防止数据丢失
6. **自动排序**：Excel 以发布日期为首列，按时间顺序排列
7. **智能重试**：请求失败自动重试（默认重试10次，带退避）
8. **磁链提取**：自动从多个源站点提取磁力链接，支持失败记录与重试
9. **按时间目录输出**：每次运行都会在 `yy-mm-dd-hh-mm/` 目录下生成独立的日志/DB/Excel/断点/失败文件
10. **标签过滤**：自动跳过“我们爱科普”“计划与日志”等非作品类标签

## 安装依赖

```bash
pip install -r requirements.txt
```

## 使用方法

### 推荐：增量更新模式（自动更新）

```bash
# 自动从第1页开始检测新帖子，只爬取未收录的内容
python vcb_scraper.py --update
```

这是**最推荐的方式**，程序会：
1. 从第1页开始串行扫描
2. 检查每个帖子是否已存在于数据库
3. 遇到已存在的页面就停止扫描
4. 只爬取新发现的帖子

### 完整爬取模式

```bash
# 爬取前10页的所有帖子
python vcb_scraper.py --max-pages 10

# 爬取所有页面（当前共163页）
python vcb_scraper.py --max-pages 163
```

### 自定义参数

```bash
# 增量更新 + 8线程 + 100帖子窗口
python vcb_scraper.py --update --workers 8 --window-size 100

# 完整爬取前50页 + 4线程 + 50帖子窗口
python vcb_scraper.py --max-pages 50 --workers 4 --window-size 50
```

### 参数说明

- `--update`：增量更新模式（推荐）
- `--max-pages`：最大爬取页数（默认163，仅在非更新模式下有效）
- `--workers`：并行线程数（默认4）
- `--window-size`：滑动窗口大小，每批处理的帖子数（默认50）
- `--retry-failed`：读取最近一次运行生成的 `scraper_failures.json`，仅重试失败的帖子

## 输出文件

程序会在仓库根目录下创建一个时间戳命名的文件夹（例如 `25-11-08-19-36/`），所有产物都会进入该目录，方便留存每次运行的快照。

### 主要文件

1. **vcb_data.db**：SQLite数据库文件（最快的存储格式）
   - 表结构：url, title, date, tags, downloads, created_at
   - 主键：url
   - 支持快速查询和更新

2. **vcb_data.xlsx**：Excel文件（方便阅读）
   - 列结构：
     * 发布日期（格式：YY-MM-DD，列在首位）
     * 页面网址
     * 标题信息（所有 `/` 字符已替换为 `&`）
     * 发布日期（格式：YY-MM-DD）
     * 标签
     * 下载格式
     * 下载格式对应的磁链
     * 下载格式2（如果有多个格式）
     * 下载格式2对应的磁链
     * 下载格式3（依此类推...）
     * 下载格式3对应的磁链
   - 按日期排序（从老到新）
   - 自动调整列宽
   - 即使磁链全部获取失败也会保留相应行，只是磁链列为空，便于后续补录

3. **vcb_scraper.log**：日志文件
   - 记录所有操作和错误信息
   - 方便调试和追踪问题

4. **scraper_checkpoint.json**：断点文件
   - 每隔10页自动保存，读取时若当前运行无断点会回溯到最近一次产物继续

5. **scraper_failures.json**：失败记录
   - 实时更新，包含帖子级失败和磁链级失败详情
   - 可配合 `--retry-failed` 针对性补录

## 数据结构

### 帖子信息

每个帖子包含以下信息：

- **url**：帖子URL
- **title**：标题
- **date**：发布日期（格式：YY-MM-DD）
- **tags**：标签（逗号分隔）
- **downloads**：下载信息列表

### 下载信息

每个下载包含：

- **format**：格式描述（如"10-bit 1080p HEVC"）
- **magnet**：磁力链接

注意：Excel 导出时，如果有多个下载格式，会分成多个列对展示：
- 下载格式、下载格式对应的磁链
- 下载格式2、下载格式2对应的磁链
- 下载格式3、下载格式3对应的磁链
- 依此类推...

## 技术特性

### 1. 多重解析策略

程序使用多种方法提取信息，确保鲁棒性：

- **标题提取**：
  1. 通过 CSS 类名 `title-article`
  2. 直接查找 h1 标签
  3. 从页面 title 标签提取

- **日期提取**：
  1. 列表页解析：在分页页面 (`/page/x`) 上先抓取日期并标准化
  2. 在帖子详情页通过 `fa-calendar` 图标定位
  3. 正则表达式匹配日期格式，并在失败时记录 HTML 片段方便排查

- **标签提取**：
  1. 列表页解析，如果已获取则不会重复访问
  2. 详情页通过 `fa-tags` 图标定位
  3. 查找 `rel="category tag"` 链接
  4. 解析失败时输出对应 HTML 片段

  带有黑名单标签（如“我们爱科普”“计划与日志”）的帖子将自动跳过，不写入数据库和导出。

- **下载信息提取**：
  1. 查找 `dw-box-download` 类
  2. 提取格式和链接

### 2. 磁链获取策略

按优先级从多个源站点获取磁链：

1. **nyaa.si**（优先级最高）
2. **share.dmhy.org**
3. **bangumi.moe**

每个链接最多尝试 1 次 HTTP 访问，但会进行最多 10 次带退避的重试（HTTP 429/超时/错误会持续重试）。解析不到磁链时，会实时写入 `scraper_failures.json`，后续可复查或使用 `--retry-failed` 重跑。

### 3. 并发控制

- 使用 `ThreadPoolExecutor` 实现并行处理
- 使用 `Lock` 防止数据库和Excel写入冲突
- 实时更新进度信息

### 4. 滑动窗口机制

为避免一次性加载过多帖子导致内存占用过大，程序采用滑动窗口批量处理：

- 默认窗口大小：50个帖子
- 每处理完一个批次，立即导出Excel
- 支持自定义窗口大小（`--window-size`）

### 5. 增量更新策略

增量更新模式的工作流程：

1. **串行扫描**：从第1页开始，逐页检查
2. **URL对比**：检查每个帖子URL是否已存在于数据库
3. **智能停止**：当某一页的所有帖子都已存在时，停止扫描
4. **批量处理**：将收集到的新帖子分批并行处理

这种方式可以：
- 快速发现最新内容
- 避免重复爬取
- 节省时间和资源

### 6. 错误处理

- 网络请求自动重试
- 异常捕获并记录到日志
- 失败的帖子不影响其他帖子处理
- 失败日志实时写入，便于随时查看和重试

## 注意事项

1. **首次运行**：首次运行建议使用完整爬取模式
   ```bash
   python vcb_scraper.py --max-pages 163 --workers 4
   ```

2. **日常更新**：之后使用增量更新模式即可
   ```bash
   python vcb_scraper.py --update
   ```

3. **网络环境**：确保能够访问 vcb-s.com 及相关下载站点

4. **并发数量**：建议根据网络状况调整 `--workers` 参数
   - 网络良好：可设置为 8-16
   - 网络一般：建议 4-8
   - 网络较差：建议 2-4

5. **重试机制**：若日志中存在失败条目，可直接运行

   ```bash
   python vcb_scraper.py --retry-failed --workers 8
   ```

   程序会自动寻找最近一次的 `scraper_failures.json` 并仅处理这些帖子。

6. **历史数据库复用**：只有在 `--update` 模式下才会自动复制最近一次运行生成的 `vcb_data.db`，用于判断哪些帖子已存在。普通完整爬取会从全新的数据库开始，以便补齐以往缺失字段。

5. **窗口大小**：根据系统内存调整 `--window-size`
   - 内存充足：100-200
   - 内存一般：50-100
   - 内存较小：20-50

6. **运行时间**：
   - 增量更新：通常只需几分钟
   - 完整爬取163页：可能需要1-3小时

7. **中断恢复**：程序实时保存数据，中断后重新运行会自动跳过已爬取的帖子

## 查询数据库

### 方法1：使用 read_db.py 脚本（推荐）

项目提供了 `read_db.py` 脚本，可以方便地查询数据库：

```bash
# 自动查找最新的数据库并显示所有帖子
python read_db.py

# 显示统计信息（总数、有磁链数量、日期范围等）
python read_db.py --stats

# 只显示前10条
python read_db.py --limit 10

# 按标签筛选（如查找包含 "1080p" 的帖子）
python read_db.py --tag 1080p

# 按标题关键词筛选
python read_db.py --title "某动画名称"

# 指定数据库路径
python read_db.py --db 25-11-08-19-36/vcb_data.db

# 不显示磁链（只显示基本信息）
python read_db.py --no-magnet

# 组合使用：查找标签为 "HEVC" 的前10条，不显示磁链
python read_db.py --tag HEVC --limit 10 --no-magnet
```

**脚本功能：**
- 自动查找最新的数据库文件（在日期格式文件夹中）
- 支持按标签、标题关键词筛选
- 显示统计信息（总数、有磁链数量、日期范围）
- 自动解析 JSON 格式的下载信息
- 格式化显示帖子详情和磁链

### 方法2：使用 SQLite 命令行工具

也可以直接使用 SQLite 客户端查询数据：

```bash
sqlite3 vcb_data.db

# 查询所有帖子
SELECT * FROM posts ORDER BY date;

# 查询特定标签的帖子
SELECT title, date FROM posts WHERE tags LIKE '%1080p%';

# 统计帖子数量
SELECT COUNT(*) FROM posts;

# 查询有磁链的帖子数量
SELECT COUNT(*) FROM posts WHERE downloads LIKE '%magnet%';
```

### 方法3：使用 Python 代码

```python
import sqlite3
import json

conn = sqlite3.connect('vcb_data.db')
cursor = conn.cursor()

# 查询所有帖子
cursor.execute("SELECT * FROM posts ORDER BY date LIMIT 10")
for row in cursor.fetchall():
    url, title, date, tags, downloads, created_at = row
    downloads_data = json.loads(downloads) if downloads else []
    print(f"{title} ({date})")
    for dl in downloads_data:
        print(f"  - {dl.get('format')}: {dl.get('magnet', '未获取')}")

conn.close()
```

## 故障排除

### 问题1：无法获取磁链

- **原因**：目标站点访问受限或网络问题
- **解决**：检查网络连接，必要时使用代理

### 问题2：Excel文件打不开

- **原因**：文件被占用或损坏
- **解决**：关闭Excel后重新运行，或删除旧文件

### 问题3：爬取速度慢

- **原因**：网络延迟或并发数太少
- **解决**：增加 `--workers` 参数值

### 问题4：部分信息缺失

- **原因**：网页结构变化或特殊页面
- **解决**：查看日志文件定位问题，可能需要更新解析规则

## 示例输出

### 增量更新模式

```
2025-11-08 10:00:00 - INFO - ============================================================
2025-11-08 10:00:00 - INFO - 运行模式：增量更新
2025-11-08 10:00:00 - INFO - 并行度: 4, 窗口大小: 50
2025-11-08 10:00:00 - INFO - ============================================================
2025-11-08 10:00:01 - INFO - 开始收集新帖子（增量更新模式）...
2025-11-08 10:00:01 - INFO - 检查第 1 页是否有新帖子...
2025-11-08 10:00:02 - INFO - 发现新帖子: https://vcb-s.com/archives/20993
2025-11-08 10:00:03 - INFO - 发现新帖子: https://vcb-s.com/archives/20974
2025-11-08 10:00:04 - INFO - 检查第 2 页是否有新帖子...
2025-11-08 10:00:05 - INFO - 第 2 页所有帖子都已存在，停止扫描
2025-11-08 10:00:05 - INFO - 共发现 2 个新帖子
2025-11-08 10:00:06 - INFO - 处理批次 1/1: 帖子 1-2/2
2025-11-08 10:00:10 - INFO - 成功提取帖子信息: Tonari no Kaibutsu-kun
2025-11-08 10:00:11 - INFO - 总进度: 1/2
2025-11-08 10:00:15 - INFO - 成功提取帖子信息: Amagami-san Chi no Enmusubi
2025-11-08 10:00:16 - INFO - 总进度: 2/2
2025-11-08 10:00:17 - INFO - 批次处理完成，已导出 Excel
2025-11-08 10:00:17 - INFO - ============================================================
2025-11-08 10:00:17 - INFO - 爬取完成！
2025-11-08 10:00:17 - INFO - 成功处理: 2 个帖子
2025-11-08 10:00:17 - INFO - 总耗时: 17.35 秒
2025-11-08 10:00:17 - INFO - 平均速度: 6.92 帖子/分钟
2025-11-08 10:00:17 - INFO - ============================================================
```

### 完整爬取模式

```
2025-11-08 10:00:00 - INFO - ============================================================
2025-11-08 10:00:00 - INFO - 运行模式：完整爬取（前 10 页）
2025-11-08 10:00:00 - INFO - 并行度: 4, 窗口大小: 50
2025-11-08 10:00:00 - INFO - ============================================================
2025-11-08 10:00:01 - INFO - 正在获取第 1 页的帖子列表
2025-11-08 10:00:02 - INFO - 第 1 页找到 8 个帖子
...
2025-11-08 10:00:20 - INFO - 共找到 80 个帖子
2025-11-08 10:00:20 - INFO - 其中 0 个已存在，80 个需要爬取
2025-11-08 10:00:21 - INFO - 处理批次 1/2: 帖子 1-50/80
...
2025-11-08 10:15:00 - INFO - ============================================================
2025-11-08 10:15:00 - INFO - 爬取完成！
2025-11-08 10:15:00 - INFO - 成功处理: 80 个帖子
2025-11-08 10:15:00 - INFO - 总耗时: 900.00 秒
2025-11-08 10:15:00 - INFO - 平均速度: 5.33 帖子/分钟
2025-11-08 10:15:00 - INFO - ============================================================
```

## 高级用法

### 定时自动更新

可以使用 cron（Linux/Mac）或任务计划程序（Windows）设置定时更新：

**Linux/Mac (crontab):**
```bash
# 每天凌晨2点自动更新
0 2 * * * cd /path/to/script && python vcb_scraper.py --update
```

**Windows (任务计划程序):**
创建基本任务，设置为每天运行，执行命令：
```
python C:\path\to\vcb_scraper.py --update
```

### 快速测试

```bash
# 只爬取第1页，用于测试
python vcb_scraper.py --max-pages 1 --workers 1
```

### 大批量处理

```bash
# 使用更大的窗口和更多线程（适合性能强劲的机器）
python vcb_scraper.py --update --workers 16 --window-size 200
```

### 内存受限环境

```bash
# 减小窗口大小，降低内存占用
python vcb_scraper.py --update --workers 2 --window-size 20
```

## 许可

本程序仅供学习交流使用，请遵守网站的使用条款。
